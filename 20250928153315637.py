#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ˜é‡‘æ–‡ç« æŠ“å–å™¨ - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬
åŠŸèƒ½ï¼š
1. æŠ“å–æ˜é‡‘æ–‡ç« å†…å®¹å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼
2. è·å–æ–‡ç« å®Œæ•´å…ƒæ•°æ®ï¼ˆä½œè€…ã€ç‚¹èµã€è¯„è®ºã€æ”¶è—ç­‰ï¼‰
3. æŠ“å–å¹¶æ’åºè¯„è®ºï¼ˆæŒ‰ç‚¹èµæ•°æ’åºï¼Œé™åˆ¶10æ¡ï¼‰
4. è·å–è¯„è®ºä¸‹çš„å­è¯„è®ºï¼ˆæœ€å¤š5æ¡ï¼‰
5. ç”Ÿæˆæ ¼å¼åŒ–çš„Markdownæ–‡ä»¶

ä½œè€…ï¼šAI Assistant
ç‰ˆæœ¬ï¼š2.0 Final
"""

import requests
from bs4 import BeautifulSoup
import html_to_markdown
import sys
import re
import os
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from typing import Dict, List, Optional, Tuple


class JuejinScraper:
    """æ˜é‡‘æ–‡ç« æŠ“å–å™¨"""
    
    def __init__(self, headless: bool = True, max_comments: int = 10, max_replies: int = 5):
        """
        åˆå§‹åŒ–æŠ“å–å™¨
        
        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            max_comments: æœ€å¤§è¯„è®ºæ•°é‡
            max_replies: æ¯æ¡è¯„è®ºä¸‹æœ€å¤§å›å¤æ•°é‡
        """
        self.headless = headless
        self.max_comments = max_comments
        self.max_replies = max_replies
        self.driver = None
    
    def setup_driver(self) -> webdriver.Chrome:
        """è®¾ç½®å¹¶è¿”å›Chrome WebDriver"""
        options = webdriver.ChromeOptions()
        if self.headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        return driver
    
    def load_comments(self, driver: webdriver.Chrome) -> None:
        """åŠ è½½æŒ‡å®šæ•°é‡çš„è¯„è®º"""
        print(f"å¼€å§‹åŠ è½½è¯„è®ºï¼Œç›®æ ‡æ•°é‡ï¼š{self.max_comments}")
        
        # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨ä»¥åŠ è½½åˆå§‹è¯„è®º
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        
        comment_count = 0
        attempts = 0
        max_attempts = 20  # æœ€å¤§å°è¯•æ¬¡æ•°
        
        while comment_count < self.max_comments and attempts < max_attempts:
            try:
                # æ£€æŸ¥å½“å‰è¯„è®ºæ•°é‡
                current_comments = driver.find_elements(By.CSS_SELECTOR, ".comment-card.comment-item")
                comment_count = len(current_comments)
                
                if comment_count >= self.max_comments:
                    print(f"å·²è¾¾åˆ°ç›®æ ‡è¯„è®ºæ•°é‡ï¼š{comment_count}")
                    break
                
                # æŸ¥æ‰¾å¹¶ç‚¹å‡»"åŠ è½½æ›´å¤š"æŒ‰é’®
                load_more_buttons = driver.find_elements(By.CSS_SELECTOR, ".fetch-more-comment")
                if load_more_buttons:
                    # ç‚¹å‡»æœ€åä¸€ä¸ªåŠ è½½æ›´å¤šæŒ‰é’®
                    last_button = load_more_buttons[-1]
                    if last_button.is_displayed() and last_button.is_enabled():
                        driver.execute_script("arguments[0].click();", last_button)
                        print(f"ç‚¹å‡»åŠ è½½æ›´å¤šï¼Œå½“å‰è¯„è®ºæ•°ï¼š{comment_count}")
                        time.sleep(3)  # å¢åŠ ç­‰å¾…æ—¶é—´
                    else:
                        print("åŠ è½½æ›´å¤šæŒ‰é’®ä¸å¯è§æˆ–ä¸å¯ç‚¹å‡»")
                        break
                else:
                    print("æ²¡æœ‰æ‰¾åˆ°æ›´å¤šè¯„è®ºåŠ è½½æŒ‰é’®")
                    break
                
                attempts += 1
                
            except Exception as e:
                print(f"åŠ è½½è¯„è®ºæ—¶å‡ºé”™ï¼š{e}")
                attempts += 1
                time.sleep(1)
        
        print(f"è¯„è®ºåŠ è½½å®Œæˆï¼Œå…±æ‰¾åˆ° {comment_count} æ¡è¯„è®º")
    
    def expand_replies(self, driver: webdriver.Chrome, comment_element) -> None:
        """å±•å¼€è¯„è®ºä¸‹çš„å›å¤"""
        try:
            # æŸ¥æ‰¾å›å¤æŒ‰é’®
            reply_buttons = comment_element.find_elements(By.CSS_SELECTOR, ".reply-btn, .show-replies")
            if reply_buttons:
                for button in reply_buttons:
                    if button.is_displayed() and button.is_enabled():
                        try:
                            driver.execute_script("arguments[0].click();", button)
                            time.sleep(1)
                            print("å±•å¼€å›å¤æˆåŠŸ")
                        except Exception as e:
                            print(f"å±•å¼€å›å¤å¤±è´¥ï¼š{e}")
                        break
        except Exception as e:
            print(f"å±•å¼€å›å¤æ—¶å‡ºé”™ï¼š{e}")
    
    def extract_replies(self, comment_element) -> List[Dict]:
        """æå–è¯„è®ºä¸‹çš„å›å¤"""
        replies = []
        try:
            # æŸ¥æ‰¾å›å¤å…ƒç´ 
            reply_elements = comment_element.find_elements(By.CSS_SELECTOR, ".reply-item, .sub-comment")
            
            for i, reply_element in enumerate(reply_elements[:self.max_replies]):
                try:
                    # æå–å›å¤ä½œè€…
                    reply_author = self._extract_reply_author(reply_element)
                    
                    # æå–å›å¤å†…å®¹
                    reply_content = self._extract_reply_content(reply_element)
                    
                    # æå–å›å¤æ—¶é—´
                    reply_time = self._extract_reply_time(reply_element)
                    
                    # æå–å›å¤ç‚¹èµæ•°
                    reply_likes = self._extract_reply_likes(reply_element)
                    
                    replies.append({
                        'author': reply_author,
                        'content': reply_content,
                        'time': reply_time,
                        'likes': reply_likes
                    })
                    
                except Exception as e:
                    print(f"å¤„ç†ç¬¬ {i+1} æ¡å›å¤æ—¶å‡ºé”™ï¼š{e}")
                    continue
                    
        except Exception as e:
            print(f"æå–å›å¤å¤±è´¥ï¼š{e}")
        
        return replies
    
    def _extract_reply_author(self, reply_element) -> str:
        """æå–å›å¤ä½œè€…å"""
        try:
            author_elements = reply_element.find_elements(By.CSS_SELECTOR, ".username .name, .reply-author")
            return author_elements[0].text.strip() if author_elements else "æœªçŸ¥ç”¨æˆ·"
        except:
            return "æœªçŸ¥ç”¨æˆ·"
    
    def _extract_reply_content(self, reply_element) -> str:
        """æå–å›å¤å†…å®¹"""
        try:
            content_elements = reply_element.find_elements(By.CSS_SELECTOR, ".reply-content, .content")
            content = content_elements[0].text.strip() if content_elements else ""
            return content.replace('\n', '\n> ')
        except:
            return ""
    
    def _extract_reply_time(self, reply_element) -> str:
        """æå–å›å¤æ—¶é—´"""
        try:
            time_elements = reply_element.find_elements(By.CSS_SELECTOR, "*[class*='time']")
            return time_elements[0].text.strip() if time_elements else "æœªçŸ¥æ—¶é—´"
        except:
            return "æœªçŸ¥æ—¶é—´"
    
    def _extract_reply_likes(self, reply_element) -> int:
        """æå–å›å¤ç‚¹èµæ•°"""
        try:
            like_elements = reply_element.find_elements(By.CSS_SELECTOR, "*[class*='digg'], *[class*='like']")
            if like_elements:
                like_text = like_elements[0].text.strip()
                like_match = re.search(r'\d+', like_text)
                return int(like_match.group()) if like_match else 0
            return 0
        except:
            return 0
    
    def extract_comments(self, driver: webdriver.Chrome) -> List[Dict]:
        """æå–è¯„è®ºæ•°æ®"""
        print("å¼€å§‹æå–è¯„è®ºä¿¡æ¯...")
        comments_data = []
        
        try:
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".comment-card.comment-item"))
            )
            
            comment_elements = driver.find_elements(By.CSS_SELECTOR, ".comment-card.comment-item")
            print(f"æ‰¾åˆ° {len(comment_elements)} æ¡è¯„è®º")
            
            for i, comment_element in enumerate(comment_elements[:self.max_comments]):
                try:
                    # å±•å¼€å›å¤
                    self.expand_replies(driver, comment_element)
                    
                    # æå–ä½œè€…å
                    author_element = comment_element.find_element(By.CSS_SELECTOR, ".username .name")
                    author = author_element.text.strip()
                    
                    # æå–è¯„è®ºå†…å®¹
                    content_element = comment_element.find_element(By.CSS_SELECTOR, ".comment-content .content")
                    content = content_element.text.strip().replace('\n', '\n> ')
                    
                    # æå–æ—¶é—´
                    time_text = self._extract_comment_time(comment_element)
                    
                    # æå–ç‚¹èµæ•° - ä¼˜åŒ–æå–é€»è¾‘
                    like_count = self._extract_comment_likes_optimized(comment_element)
                    
                    # æå–å›å¤æ•° - ä¼˜åŒ–æå–é€»è¾‘
                    reply_count = self._extract_comment_replies_optimized(comment_element)
                    
                    # æå–å­è¯„è®º
                    replies = self.extract_replies(comment_element)
                    
                    comments_data.append({
                        'author': author,
                        'content': content,
                        'time': time_text,
                        'likes': like_count,
                        'replies': reply_count,
                        'sub_replies': replies
                    })
                    
                    print(f"å¤„ç†ç¬¬ {i+1} æ¡è¯„è®ºï¼š{author} - ç‚¹èµ:{like_count} å›å¤:{reply_count} å­å›å¤:{len(replies)}")
                    
                except Exception as e:
                    print(f"å¤„ç†ç¬¬ {i+1} æ¡è¯„è®ºæ—¶å‡ºé”™ï¼š{e}")
                    continue
                    
        except Exception as e:
            print(f"æå–è¯„è®ºå¤±è´¥ï¼š{e}")
        
        return comments_data
    
    def _extract_comment_time(self, comment_element) -> str:
        """æå–è¯„è®ºæ—¶é—´"""
        try:
            time_elements = comment_element.find_elements(By.CSS_SELECTOR, "*[class*='time']")
            return time_elements[0].text.strip() if time_elements else "æœªçŸ¥æ—¶é—´"
        except:
            return "æœªçŸ¥æ—¶é—´"
    
    def _extract_comment_likes_optimized(self, comment_element) -> int:
        """ä¼˜åŒ–æå–è¯„è®ºç‚¹èµæ•°"""
        try:
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾ç‚¹èµæŒ‰é’®ä¸Šçš„æ•°å­—
            like_buttons = comment_element.find_elements(By.CSS_SELECTOR, ".like-btn, .digg-btn, [class*='like'], [class*='digg']")
            for button in like_buttons:
                try:
                    # æŸ¥æ‰¾æŒ‰é’®å†…çš„æ•°å­—æ–‡æœ¬
                    button_text = button.text.strip()
                    if button_text and button_text.isdigit():
                        return int(button_text)
                    
                    # æŸ¥æ‰¾æŒ‰é’®çš„dataå±æ€§
                    data_likes = button.get_attribute("data-likes") or button.get_attribute("data-count")
                    if data_likes and data_likes.isdigit():
                        return int(data_likes)
                        
                except:
                    continue
            
            # æ–¹æ³•2ï¼šæŸ¥æ‰¾åŒ…å«ç‚¹èµæ•°çš„spanå…ƒç´ 
            like_spans = comment_element.find_elements(By.CSS_SELECTOR, "span[class*='count'], span[class*='num'], span[class*='like']")
            for span in like_spans:
                try:
                    span_text = span.text.strip()
                    if span_text and span_text.isdigit():
                        return int(span_text)
                except:
                    continue
            
            # æ–¹æ³•3ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ•°å­—çš„æ–‡æœ¬ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºç‚¹èµæ•°
            all_text = comment_element.text
            like_patterns = [
                r'ç‚¹èµ\s*(\d+)',
                r'(\d+)\s*èµ',
                r'(\d+)\s*like',
                r'like\s*(\d+)'
            ]
            
            for pattern in like_patterns:
                match = re.search(pattern, all_text, re.IGNORECASE)
                if match:
                    return int(match.group(1))
            
            return 0
            
        except Exception as e:
            print(f"æå–ç‚¹èµæ•°æ—¶å‡ºé”™ï¼š{e}")
            return 0
    
    def _extract_comment_replies_optimized(self, comment_element) -> int:
        """ä¼˜åŒ–æå–è¯„è®ºå›å¤æ•°"""
        try:
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾å›å¤æŒ‰é’®ä¸Šçš„æ•°å­—
            reply_buttons = comment_element.find_elements(By.CSS_SELECTOR, ".reply-btn, .show-replies, [class*='reply']")
            for button in reply_buttons:
                try:
                    button_text = button.text.strip()
                    if button_text and button_text.isdigit():
                        return int(button_text)
                    
                    # æŸ¥æ‰¾æŒ‰é’®çš„dataå±æ€§
                    data_replies = button.get_attribute("data-replies") or button.get_attribute("data-count")
                    if data_replies and data_replies.isdigit():
                        return int(data_replies)
                        
                except:
                    continue
            
            # æ–¹æ³•2ï¼šæŸ¥æ‰¾åŒ…å«å›å¤æ•°çš„spanå…ƒç´ 
            reply_spans = comment_element.find_elements(By.CSS_SELECTOR, "span[class*='count'], span[class*='num'], span[class*='reply']")
            for span in reply_spans:
                try:
                    span_text = span.text.strip()
                    if span_text and span_text.isdigit():
                        return int(span_text)
                except:
                    continue
            
            # æ–¹æ³•3ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ•°å­—çš„æ–‡æœ¬ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºå›å¤æ•°
            all_text = comment_element.text
            reply_patterns = [
                r'å›å¤\s*(\d+)',
                r'(\d+)\s*å›å¤',
                r'(\d+)\s*reply',
                r'reply\s*(\d+)'
            ]
            
            for pattern in reply_patterns:
                match = re.search(pattern, all_text, re.IGNORECASE)
                if match:
                    return int(match.group(1))
            
            return 0
            
        except Exception as e:
            print(f"æå–å›å¤æ•°æ—¶å‡ºé”™ï¼š{e}")
            return 0
    
    def extract_author_info(self, driver: webdriver.Chrome) -> Tuple[str, str]:
        """æå–ä½œè€…ä¿¡æ¯"""
        print("æå–ä½œè€…ä¿¡æ¯...")
        
        # æ–¹æ³•1ï¼šæŸ¥æ‰¾æ‰€æœ‰ç”¨æˆ·é“¾æ¥
        try:
            user_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='/user/']")
            print(f"æ‰¾åˆ° {len(user_links)} ä¸ªç”¨æˆ·é“¾æ¥")
            
            for link in user_links:
                try:
                    href = link.get_attribute("href")
                    if href and "/user/" in href and "posts" in href:
                        try:
                            name_element = link.find_element(By.CSS_SELECTOR, ".name, .username")
                            author_name = name_element.text.strip()
                            if author_name:
                                if href.startswith('/'):
                                    href = "https://juejin.cn" + href
                                print(f"æ‰¾åˆ°ä½œè€…ï¼š{author_name}")
                                return author_name, href
                        except:
                            continue
                except:
                    continue
        except Exception as e:
            print(f"æ–¹æ³•1æå–ä½œè€…ä¿¡æ¯å¤±è´¥ï¼š{e}")
        
        # æ–¹æ³•2ï¼šå¤‡ç”¨æ–¹æ³•
        try:
            author_element = driver.find_element(By.CSS_SELECTOR, ".user-name, .username, .author-name")
            author_name = author_element.text.strip()
            print(f"å¤‡ç”¨æ–¹æ³•æ‰¾åˆ°ä½œè€…ï¼š{author_name}")
            return author_name, ""
        except Exception as e:
            print(f"å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ï¼š{e}")
            return "æœªçŸ¥ä½œè€…", ""
    
    def extract_article_stats(self, driver: webdriver.Chrome) -> Dict[str, int]:
        """æå–æ–‡ç« ç»Ÿè®¡æ•°æ®ï¼ˆç‚¹èµã€è¯„è®ºã€æ”¶è—ï¼‰"""
        print("æå–æ–‡ç« ç»Ÿè®¡æ•°æ®...")
        stats = {'likes': 0, 'comments': 0, 'collects': 0}
        
        try:
            panel_buttons = driver.find_elements(By.CSS_SELECTOR, ".panel-btn.with-badge")
            print(f"æ‰¾åˆ° {len(panel_buttons)} ä¸ªç»Ÿè®¡æŒ‰é’®")
            
            for button in panel_buttons:
                try:
                    badge_value = button.get_attribute("badge")
                    if not badge_value:
                        continue
                    
                    svg_element = button.find_element(By.CSS_SELECTOR, "svg")
                    svg_class = svg_element.get_attribute("class")
                    
                    if "icon-zan" in svg_class:
                        stats['likes'] = int(badge_value)
                        print(f"ç‚¹èµæ•°ï¼š{stats['likes']}")
                    elif "icon-comment" in svg_class:
                        stats['comments'] = int(badge_value)
                        print(f"è¯„è®ºæ•°ï¼š{stats['comments']}")
                    elif "icon-collect" in svg_class:
                        stats['collects'] = int(badge_value)
                        print(f"æ”¶è—æ•°ï¼š{stats['collects']}")
                        
                except Exception as e:
                    print(f"å¤„ç†ç»Ÿè®¡æŒ‰é’®æ—¶å‡ºé”™ï¼š{e}")
                    continue
            
        except Exception as e:
            print(f"æå–ç»Ÿè®¡æ•°æ®å¤±è´¥ï¼š{e}")
        
        return stats
    
    def extract_additional_metadata(self, driver: webdriver.Chrome) -> Dict[str, str]:
        """æå–é¢å¤–çš„å…ƒæ•°æ®"""
        metadata = {}
        
        # æå–å‘è¡¨æ—¶é—´
        try:
            time_element = driver.find_element(By.CSS_SELECTOR, "*[class*='time']")
            metadata['publish_time'] = time_element.text.strip()
            print(f"å‘è¡¨æ—¶é—´ï¼š{metadata['publish_time']}")
        except:
            metadata['publish_time'] = "æœªçŸ¥æ—¶é—´"
        
        # æå–é˜…è¯»æ—¶é•¿
        try:
            page_text = driver.execute_script("return document.body.innerText;")
            read_match = re.search(r'é˜…è¯»(\d+åˆ†é’Ÿ)', page_text)
            metadata['read_time'] = read_match.group(1) if read_match else "æœªçŸ¥"
            print(f"é˜…è¯»æ—¶é•¿ï¼š{metadata['read_time']}")
        except:
            metadata['read_time'] = "æœªçŸ¥"
        
        # æå–ä¸“æ åç§°
        try:
            column_elements = driver.find_elements(By.XPATH, "//*[contains(text(), 'ä¸“æ ')]")
            if column_elements:
                for elem in column_elements:
                    text = elem.text.strip()
                    if 'ä¸“æ ' in text and len(text) < 50:
                        metadata['column'] = text
                        break
                else:
                    metadata['column'] = "æ— ä¸“æ "
            else:
                metadata['column'] = "æ— ä¸“æ "
            print(f"ä¸“æ åç§°ï¼š{metadata['column']}")
        except:
            metadata['column'] = "æ— ä¸“æ "
        
        return metadata
    
    def generate_markdown(self, article_data: Dict) -> str:
        """ç”ŸæˆMarkdownå†…å®¹"""
        md_content = []
        
        # æ ‡é¢˜
        md_content.append(f"# {article_data['title']}\n")
        
        # ä½œè€…ä¿¡æ¯
        if article_data['author_link']:
            md_content.append(f"**ä½œè€…ï¼š** [{article_data['author_name']}]({article_data['author_link']})\n")
        else:
            md_content.append(f"**ä½œè€…ï¼š** {article_data['author_name']}\n")
        
        # æ–‡ç« ä¿¡æ¯è¡¨æ ¼
        md_content.append("## ğŸ“Š æ–‡ç« ä¿¡æ¯\n")
        md_content.append("| é¡¹ç›® | å†…å®¹ |")
        md_content.append("|------|------|")
        md_content.append(f"| å‘è¡¨æ—¶é—´ | {article_data['publish_time']} |")
        md_content.append(f"| ç‚¹èµæ•° | {article_data['likes']} |")
        md_content.append(f"| è¯„è®ºæ•° | {article_data['comments']} |")
        md_content.append(f"| æ”¶è—æ•° | {article_data['collects']} |")
        md_content.append(f"| é˜…è¯»æ—¶é•¿ | {article_data['read_time']} |")
        md_content.append(f"| ä¸“æ åç§° | {article_data['column']} |")
        md_content.append(f"| åŸæ–‡é“¾æ¥ | [{article_data['title']}]({article_data['url']}) |\n")
        
        md_content.append("---\n")
        
        # æ–‡ç« å†…å®¹
        md_content.append("## ğŸ“ æ–‡ç« å†…å®¹\n")
        md_content.append(article_data['content'])
        
        # ç²¾é€‰è¯„è®º
        if article_data['comments_data']:
            md_content.append("\n\n---\n")
            md_content.append("## ğŸ’¬ ç²¾é€‰è¯„è®º\n")
            
            # æŒ‰ç‚¹èµæ•°æ’åºå¹¶å–å‰10æ¡
            sorted_comments = sorted(article_data['comments_data'], key=lambda x: x['likes'], reverse=True)
            top_comments = sorted_comments[:self.max_comments]
            
            for comment in top_comments:
                # æ„å»ºè¯„è®ºæ ‡é¢˜ï¼Œå§‹ç»ˆæ˜¾ç¤ºç‚¹èµå’Œå›å¤æ•°
                title_parts = [
                    comment['author'],
                    f"ğŸ‘ {comment['likes']}",
                    f"ğŸ’¬ {comment['replies']}",
                    comment['time']
                ]
                comment_title = " ".join(title_parts)
                md_content.append(f"### {comment_title}\n")
                md_content.append(f"{comment['content']}\n")
                
                # æ˜¾ç¤ºå­è¯„è®º
                if comment['sub_replies']:
                    md_content.append("\n**å›å¤ï¼š**\n")
                    for reply in comment['sub_replies']:
                        # å§‹ç»ˆæ˜¾ç¤ºå­è¯„è®ºçš„ç‚¹èµæ•°
                        reply_title = f"{reply['author']} (ğŸ‘ {reply['likes']}) - {reply['time']}"
                        
                        md_content.append(f"**{reply_title}**\n")
                        md_content.append(f"> {reply['content']}\n")
                
                md_content.append("\n---\n")
        
        return "\n".join(md_content)
    
    def save_article(self, url: str) -> Optional[str]:
        """
        æŠ“å–å¹¶ä¿å­˜æ–‡ç« 
        
        Args:
            url: æ–‡ç« URL
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            self.driver = self.setup_driver()
            self.driver.get(url)
            
            # ç­‰å¾…æ–‡ç« åŠ è½½
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "article-root"))
            )
            
            print(f"å¼€å§‹å¤„ç†æ–‡ç« ï¼š{url}")
            
            # åŠ è½½è¯„è®º
            self.load_comments(self.driver)
            
            # æå–è¯„è®ºæ•°æ®
            comments_data = self.extract_comments(self.driver)
            
            # è·å–é¡µé¢æºç ç”¨äºBeautifulSoupè§£æ
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # æå–æ–‡ç« æ ‡é¢˜
            title_tag = soup.find('h1', class_='article-title') or soup.find('title')
            if not title_tag:
                print("é”™è¯¯ï¼šæ— æ³•æ‰¾åˆ°æ–‡ç« æ ‡é¢˜")
                return None
            
            title = title_tag.get_text().strip()
            print(f"æ–‡ç« æ ‡é¢˜ï¼š{title}")
            
            # æå–ä½œè€…ä¿¡æ¯
            author_name, author_link = self.extract_author_info(self.driver)
            
            # æå–æ–‡ç« ç»Ÿè®¡æ•°æ®
            stats = self.extract_article_stats(self.driver)
            
            # æå–é¢å¤–å…ƒæ•°æ®
            metadata = self.extract_additional_metadata(self.driver)
            
            # æå–æ–‡ç« å†…å®¹
            article_container = soup.find(id='article-root')
            if not article_container:
                print("é”™è¯¯ï¼šæ— æ³•æ‰¾åˆ°æ–‡ç« å†…å®¹")
                return None
            
            # Remove decorative code block elements
            for header in article_container.find_all("div", class_="code-block-extension-header"):
                header.decompose()

            # Move code from <code> to <pre> to avoid extra newlines
            for pre in article_container.find_all('pre'):
                if pre.code:
                    pre.string = pre.code.get_text().strip()
            
            markdown_content = html_to_markdown.markdownify(str(article_container))
            
            # å‡†å¤‡æ–‡ç« æ•°æ®
            article_data = {
                'title': title,
                'url': url,
                'author_name': author_name,
                'author_link': author_link,
                'content': markdown_content,
                'comments_data': comments_data,
                **stats,
                **metadata
            }
            
            # ç”ŸæˆMarkdown
            final_markdown = self.generate_markdown(article_data)
            
            # ä¿å­˜æ–‡ä»¶
            safe_filename = re.sub(r'[\/*?"<>|]', "", title)
            safe_filename = safe_filename.replace(' ', '_') + ".md"
            save_path = os.path.expanduser(f"~/{safe_filename}")
            
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(final_markdown)
            
            print(f"âœ… æ–‡ç« å·²ä¿å­˜åˆ°ï¼š{save_path}")
            return save_path
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ç« æ—¶å‡ºé”™ï¼š{e}")
            return None
        
        finally:
            if self.driver:
                self.driver.quit()


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•ï¼špython juejin_scraper_final.py <URL1> [URL2] ...")
        print("ç¤ºä¾‹ï¼špython juejin_scraper_final.py https://juejin.cn/post/7511582195447824438")
        sys.exit(1)
    
    scraper = JuejinScraper(headless=True, max_comments=10, max_replies=5)
    
    urls = sys.argv[1:]
    success_count = 0
    
    print(f"ğŸ“š å¼€å§‹å¤„ç† {len(urls)} ç¯‡æ–‡ç« ...")
    print("=" * 50)
    
    for i, url in enumerate(urls, 1):
        print(f"\nğŸ”„ [{i}/{len(urls)}] å¤„ç†æ–‡ç« ...")
        result = scraper.save_article(url)
        
        if result:
            success_count += 1
            print(f"âœ… ç¬¬ {i} ç¯‡æ–‡ç« å¤„ç†å®Œæˆ")
        else:
            print(f"âŒ ç¬¬ {i} ç¯‡æ–‡ç« å¤„ç†å¤±è´¥")
        
        print("-" * 30)
    
    print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼æˆåŠŸï¼š{success_count}/{len(urls)} ç¯‡æ–‡ç« ")


if __name__ == "__main__":
    main() 